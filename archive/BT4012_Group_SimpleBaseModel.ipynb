{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost with No feature engineering"
      ],
      "metadata": {
        "id": "TINkjn93zHyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        ")\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fraudulent Data (5)/Fraudulent_E-Commerce_Transaction_Data_MERGED.csv')\n",
        "\n",
        "print(\"Starting SIMPLE XGBOOST MODEL (No Feature Engineering)\")\n",
        "\n",
        "# --- 2. Remove columns not usable directly ---\n",
        "cols_to_drop = [\n",
        "    'Transaction ID',\n",
        "    'Customer ID',\n",
        "    'Shipping Address',\n",
        "    'Billing Address',\n",
        "    'Customer Location',\n",
        "    'IP Address',\n",
        "    'Transaction Date'      # datetime not used\n",
        "]\n",
        "\n",
        "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "# --- 3. Handle Missing Values ---\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_cols = [c for c in numeric_cols if c != 'Is Fraudulent']  # exclude target\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# numeric → median\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# categorical → \"Unknown\"\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "# --- 4. Encode categorical variables (simple, no OHE) ---\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- 5. Prepare features ---\n",
        "X = df.drop(columns=['Is Fraudulent'])\n",
        "y = df['Is Fraudulent']\n",
        "\n",
        "# --- 6. Train / Validation / Test Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# validation split from training\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.20, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "# --- 7. Train Simple XGBoost Model ---\n",
        "model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    n_estimators=200,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 8. Validation Metrics ---\n",
        "val_pred = model.predict(X_val)\n",
        "val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"\\n--- VALID METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, val_pred))\n",
        "print(\"Precision:\", precision_score(y_val, val_pred))\n",
        "print(\"Recall:\", recall_score(y_val, val_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, val_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_val, val_proba))\n",
        "\n",
        "# --- 9. Test Metrics ---\n",
        "test_pred = model.predict(X_test)\n",
        "test_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- TEST METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"Precision:\", precision_score(y_test, test_pred))\n",
        "print(\"Recall:\", recall_score(y_test, test_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, test_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_test, test_proba))\n",
        "\n",
        "print(\"\\n--- CLASSIFICATION REPORT (Test) ---\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vax2V-k1oJVf",
        "outputId": "36575bbd-55b9-4e55-f112-1da02f6be74a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SIMPLE XGBOOST MODEL (No Feature Engineering)\n",
            "Train: 838088, Validation: 209522, Test: 448976\n",
            "\n",
            "--- VALID METRICS ---\n",
            "Accuracy: 0.9558232548372009\n",
            "Precision: 0.7936210131332082\n",
            "Recall: 0.16102017510468214\n",
            "F1 Score: 0.26772151898734176\n",
            "AUC: 0.8177930205046777\n",
            "\n",
            "--- TEST METRICS ---\n",
            "Accuracy: 0.9554029614055094\n",
            "Precision: 0.7828156880525958\n",
            "Recall: 0.15334399147348787\n",
            "F1 Score: 0.2564521519551413\n",
            "AUC: 0.8191914196349335\n",
            "\n",
            "--- CLASSIFICATION REPORT (Test) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98    426458\n",
            "           1       0.78      0.15      0.26     22518\n",
            "\n",
            "    accuracy                           0.96    448976\n",
            "   macro avg       0.87      0.58      0.62    448976\n",
            "weighted avg       0.95      0.96      0.94    448976\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForest with no feature engineering"
      ],
      "metadata": {
        "id": "Xh0bZCjUzMbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        ")\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fraudulent Data (5)/Fraudulent_E-Commerce_Transaction_Data_MERGED.csv')\n",
        "\n",
        "print(\"Starting SIMPLE RANDOM FOREST MODEL (No Feature Engineering)\")\n",
        "\n",
        "# --- 2. Remove unusable columns ---\n",
        "cols_to_drop = [\n",
        "    'Transaction ID',\n",
        "    'Customer ID',\n",
        "    'Shipping Address',\n",
        "    'Billing Address',\n",
        "    'Customer Location',\n",
        "    'IP Address',\n",
        "    'Transaction Date'      # datetime kept out since no feature engineering\n",
        "]\n",
        "\n",
        "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "# --- 3. Handle Missing Values ---\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_cols = [c for c in numeric_cols if c != 'Is Fraudulent']  # exclude target\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# numeric → median\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# categorical → \"Unknown\"\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "# --- 4. Encode categorical variables (simple label encoding) ---\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- 5. Prepare features ---\n",
        "X = df.drop(columns=['Is Fraudulent'])\n",
        "y = df['Is Fraudulent']\n",
        "\n",
        "# --- 6. Train / Validation / Test Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Secondary validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.20, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "# --- 7. Train Random Forest ---\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    class_weight=\"balanced\",     # handles imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 8. Validation Metrics ---\n",
        "val_pred = model.predict(X_val)\n",
        "val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"\\n--- VALID METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, val_pred))\n",
        "print(\"Precision:\", precision_score(y_val, val_pred))\n",
        "print(\"Recall:\", recall_score(y_val, val_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, val_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_val, val_proba))\n",
        "\n",
        "# --- 9. Test Metrics ---\n",
        "test_pred = model.predict(X_test)\n",
        "test_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- TEST METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"Precision:\", precision_score(y_test, test_pred))\n",
        "print(\"Recall:\", recall_score(y_test, test_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, test_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_test, test_proba))\n",
        "\n",
        "print(\"\\n--- CLASSIFICATION REPORT (Test) ---\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EA1a8fWrCTw",
        "outputId": "96717b81-f67e-455f-80a5-d4af2ee7e279"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SIMPLE RANDOM FOREST MODEL (No Feature Engineering)\n",
            "Train: 838088, Validation: 209522, Test: 448976\n",
            "\n",
            "--- VALID METRICS ---\n",
            "Accuracy: 0.9553698418304521\n",
            "Precision: 0.7727487034417727\n",
            "Recall: 0.15597639893414542\n",
            "F1 Score: 0.2595613271042838\n",
            "AUC: 0.7933515892466723\n",
            "\n",
            "--- TEST METRICS ---\n",
            "Accuracy: 0.9549218666476604\n",
            "Precision: 0.7629817678282945\n",
            "Recall: 0.14681588062883028\n",
            "F1 Score: 0.2462478119995531\n",
            "AUC: 0.7933679585451869\n",
            "\n",
            "--- CLASSIFICATION REPORT (Test) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98    426458\n",
            "           1       0.76      0.15      0.25     22518\n",
            "\n",
            "    accuracy                           0.95    448976\n",
            "   macro avg       0.86      0.57      0.61    448976\n",
            "weighted avg       0.95      0.95      0.94    448976\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression with no feature engineering"
      ],
      "metadata": {
        "id": "gwTJ_fhjzRpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        ")\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fraudulent Data (5)/Fraudulent_E-Commerce_Transaction_Data_MERGED.csv')\n",
        "\n",
        "print(\"Starting SIMPLE MODEL (No Feature Engineering)\")\n",
        "\n",
        "# --- 2. Remove columns that cannot be used directly ---\n",
        "\n",
        "cols_to_drop = [\n",
        "    'Transaction ID',\n",
        "    'Customer ID',\n",
        "    'Shipping Address',\n",
        "    'Billing Address',\n",
        "    'Customer Location',\n",
        "    'IP Address',\n",
        "    'Transaction Date'     # raw datetime cannot be used directly\n",
        "]\n",
        "\n",
        "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "# Keep Transaction Hour and Transaction Amount as-is\n",
        "# Keep categorical variables but encode them later\n",
        "\n",
        "# --- 3. Handle Missing Values ---\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_cols = [c for c in numeric_cols if c != 'Is Fraudulent']\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "# --- 4. Encode Categorical Columns (Very simple, no OHE) ---\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- 5. Prepare X and y ---\n",
        "\n",
        "X = df.drop(columns=['Is Fraudulent'])\n",
        "y = df['Is Fraudulent']\n",
        "\n",
        "# --- 6. Train / Validation / Test Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Make a validation split from training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.20, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "# --- 7. Train a SIMPLE ML MODEL (Logistic Regression) ---\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 8. Validation Metrics ---\n",
        "\n",
        "val_pred = model.predict(X_val)\n",
        "val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"\\n--- VALIDATION METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_val, val_pred))\n",
        "print(\"Precision:\", precision_score(y_val, val_pred))\n",
        "print(\"Recall:\", recall_score(y_val, val_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, val_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_val, val_proba))\n",
        "\n",
        "# --- 9. Test Metrics ---\n",
        "\n",
        "test_pred = model.predict(X_test)\n",
        "test_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- TEST METRICS ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"Precision:\", precision_score(y_test, test_pred))\n",
        "print(\"Recall:\", recall_score(y_test, test_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, test_pred))\n",
        "print(\"AUC:\", roc_auc_score(y_test, test_proba))\n",
        "\n",
        "print(\"\\n--- CLASSIFICATION REPORT (Test) ---\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_s0E6H9u9Sf",
        "outputId": "bc80dc3e-7816-4df5-e222-63f33cf5adf6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SIMPLE MODEL (No Feature Engineering)\n",
            "Train: 838088, Validation: 209522, Test: 448976\n",
            "\n",
            "--- VALIDATION METRICS ---\n",
            "Accuracy: 0.9547684730004486\n",
            "Precision: 0.8622628250175686\n",
            "Recall: 0.11676817662733156\n",
            "F1 Score: 0.20568267538345486\n",
            "AUC: 0.7677108293295923\n",
            "\n",
            "--- TEST METRICS ---\n",
            "Accuracy: 0.9546456826200064\n",
            "Precision: 0.8704022000687521\n",
            "Recall: 0.11244337863042898\n",
            "F1 Score: 0.1991583749557557\n",
            "AUC: 0.7715598183251018\n",
            "\n",
            "--- CLASSIFICATION REPORT (Test) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98    426458\n",
            "           1       0.87      0.11      0.20     22518\n",
            "\n",
            "    accuracy                           0.95    448976\n",
            "   macro avg       0.91      0.56      0.59    448976\n",
            "weighted avg       0.95      0.95      0.94    448976\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initial feature engineering with XGBoost (NOT FINAL MODEL)"
      ],
      "metadata": {
        "id": "fQ4K0_vgzXRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD DATA\n",
        "# ============================================================\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Fraudulent Data (5)/Fraudulent_E-Commerce_Transaction_Data_MERGED.csv')\n",
        "\n",
        "print(\"--- Starting Feature Engineering ---\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "# Convert date + hour → timestamp\n",
        "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
        "df['Transaction_DateTime'] = df['Transaction Date'] + pd.to_timedelta(df['Transaction Hour'], unit='H')\n",
        "\n",
        "df = df.sort_values(by='Transaction_DateTime').reset_index(drop=True)\n",
        "\n",
        "# Handle duplicates\n",
        "initial_rows = len(df)\n",
        "df = df.drop_duplicates(subset=['Transaction ID'], keep='first')\n",
        "print(f\"Dropped {initial_rows - len(df)} duplicate transaction records.\")\n",
        "\n",
        "# Handle missing values\n",
        "numerical_cols = ['Transaction Amount', 'Customer Age']\n",
        "categorical_cols = ['Payment Method', 'Product Category', 'Device Used']\n",
        "\n",
        "for col in numerical_cols:\n",
        "    median_val = df[col].median()\n",
        "    df[col].fillna(median_val, inplace=True)\n",
        "    print(f\"Imputed NaN values in '{col}' with median: {median_val}\")\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna('Unknown', inplace=True)\n",
        "    print(f\"Imputed NaN values in '{col}' with 'Unknown'\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. ONE-HOT ENCODING\n",
        "# ============================================================\n",
        "\n",
        "df = pd.get_dummies(\n",
        "    df,\n",
        "    columns=['Payment Method', 'Device Used', 'Product Category'],\n",
        "    prefix=['PM', 'Device', 'Product'],\n",
        "    dtype=int\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. TEMPORAL FEATURES\n",
        "# ============================================================\n",
        "\n",
        "df['Transaction_Year'] = df['Transaction_DateTime'].dt.year\n",
        "df['Transaction_Month'] = df['Transaction_DateTime'].dt.month\n",
        "df['Transaction_Day_Of_Week'] = df['Transaction_DateTime'].dt.dayofweek\n",
        "df['Transaction_Hour_Of_Day'] = df['Transaction_DateTime'].dt.hour\n",
        "df['Is_Weekend'] = df['Transaction_Day_Of_Week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. VELOCITY FEATURES\n",
        "# ============================================================\n",
        "\n",
        "def calculate_velocity_features(df_input, group_col, time_col, window, agg_col='Transaction Amount'):\n",
        "    df_temp = df_input.copy()\n",
        "    grouped = df_temp.groupby(group_col)\n",
        "\n",
        "    count_series = grouped.rolling(window=window, min_periods=1, on=time_col)[agg_col] \\\n",
        "                        .count().shift(1).fillna(0)\n",
        "\n",
        "    sum_series = grouped.rolling(window=window, min_periods=1, on=time_col)[agg_col] \\\n",
        "                        .sum().shift(1).fillna(0)\n",
        "\n",
        "    mean_series = grouped.rolling(window=window, min_periods=1, on=time_col)[agg_col] \\\n",
        "                         .mean().shift(1).fillna(0)\n",
        "\n",
        "    # Reset index for merging\n",
        "    count_series = count_series.reset_index(level=0, drop=True)\n",
        "    sum_series = sum_series.reset_index(level=0, drop=True)\n",
        "    mean_series = mean_series.reset_index(level=0, drop=True)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        f'{group_col}_Tx_Count_Last_{window}': count_series,\n",
        "        f'{group_col}_Amount_Sum_Last_{window}': sum_series,\n",
        "        f'{group_col}_Amount_Mean_Last_{window}': mean_series\n",
        "    })\n",
        "\n",
        "velocity_1d = calculate_velocity_features(df, 'Customer ID', 'Transaction_DateTime', '1D')\n",
        "velocity_7d = calculate_velocity_features(df, 'Customer ID', 'Transaction_DateTime', '7D')\n",
        "\n",
        "df = pd.concat([df.reset_index(drop=True),\n",
        "                velocity_1d.reset_index(drop=True),\n",
        "                velocity_7d.reset_index(drop=True)], axis=1)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. IDENTITY + ACCOUNT FEATURES\n",
        "# ============================================================\n",
        "\n",
        "df['Address_Match'] = (df['Shipping Address'] == df['Billing Address']).astype(int)\n",
        "df['Customer_Tx_Count_Total'] = df.groupby('Customer ID')['Transaction ID'].transform('count')\n",
        "df['Account_Age_to_Tx_Ratio'] = df['Account Age Days'] / df['Customer_Tx_Count_Total']\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. FEATURE SELECTION\n",
        "# ============================================================\n",
        "\n",
        "X = df.drop(columns=[\n",
        "    'Transaction ID', 'Customer ID', 'Transaction Date', 'Transaction Hour',\n",
        "    'Shipping Address', 'Billing Address', 'Is Fraudulent',\n",
        "    'Transaction_Day_Of_Week', 'Transaction_DateTime',\n",
        "    'Customer_Tx_Count_Total', 'Customer Location', 'IP Address'\n",
        "])\n",
        "\n",
        "y = df['Is Fraudulent']\n",
        "\n",
        "print(f\"\\nTotal Features: {X.shape[1]}\")\n",
        "print(f\"Transactions: {len(df)}\")\n",
        "print(f\"Fraudulent Cases: {y.sum()}\")\n",
        "print(f\"Non-Fraudulent Cases: {len(y)-y.sum()}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. TRAIN/TEST SPLIT + VALIDATION SPLIT\n",
        "# ============================================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.20, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9. TRAIN XGBOOST\n",
        "# ============================================================\n",
        "\n",
        "model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='auc',\n",
        "    random_state=42,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1])\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10. VALIDATION METRICS\n",
        "# ============================================================\n",
        "\n",
        "val_pred = model.predict(X_val)\n",
        "val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"\\n--- VALIDATION METRICS ---\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, val_pred))\n",
        "print(\"Precision:\", precision_score(y_val, val_pred))\n",
        "print(\"Recall   :\", recall_score(y_val, val_pred))\n",
        "print(\"F1 Score :\", f1_score(y_val, val_pred))\n",
        "print(\"AUC      :\", roc_auc_score(y_val, val_proba))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 11. TEST METRICS\n",
        "# ============================================================\n",
        "\n",
        "test_pred = model.predict(X_test)\n",
        "test_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- TEST METRICS ---\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, test_pred))\n",
        "print(\"Precision:\", precision_score(y_test, test_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, test_pred))\n",
        "print(\"F1 Score :\", f1_score(y_test, test_pred))\n",
        "print(\"AUC      :\", roc_auc_score(y_test, test_proba))\n",
        "\n",
        "print(\"\\n--- CLASSIFICATION REPORT (Test) ---\")\n",
        "print(classification_report(y_test, test_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mERc_xIxyXk_",
        "outputId": "e6f081b7-1b7f-4495-c712-b9cc48955a77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Feature Engineering ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3560064797.py:29: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  df['Transaction_DateTime'] = df['Transaction Date'] + pd.to_timedelta(df['Transaction Hour'], unit='H')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 0 duplicate transaction records.\n",
            "Imputed NaN values in 'Transaction Amount' with median: 151.76\n",
            "Imputed NaN values in 'Customer Age' with median: 35.0\n",
            "Imputed NaN values in 'Payment Method' with 'Unknown'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3560064797.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(median_val, inplace=True)\n",
            "/tmp/ipython-input-3560064797.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna('Unknown', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputed NaN values in 'Product Category' with 'Unknown'\n",
            "Imputed NaN values in 'Device Used' with 'Unknown'\n",
            "\n",
            "Total Features: 28\n",
            "Transactions: 1496586\n",
            "Fraudulent Cases: 75060\n",
            "Non-Fraudulent Cases: 1421526\n",
            "Train: 838088, Validation: 209522, Test: 448976\n",
            "\n",
            "--- VALIDATION METRICS ---\n",
            "Accuracy : 0.878595087866668\n",
            "Precision: 0.2399038294017213\n",
            "Recall   : 0.6552150742291587\n",
            "F1 Score : 0.3512127936337899\n",
            "AUC      : 0.810318095992076\n",
            "\n",
            "--- TEST METRICS ---\n",
            "Accuracy : 0.8785102098998611\n",
            "Precision: 0.24044539531265194\n",
            "Recall   : 0.6588062883026912\n",
            "F1 Score : 0.3523083499572528\n",
            "AUC      : 0.811967828154596\n",
            "\n",
            "--- CLASSIFICATION REPORT (Test) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93    426458\n",
            "           1       0.24      0.66      0.35     22518\n",
            "\n",
            "    accuracy                           0.88    448976\n",
            "   macro avg       0.61      0.77      0.64    448976\n",
            "weighted avg       0.94      0.88      0.90    448976\n",
            "\n"
          ]
        }
      ]
    }
  ]
}